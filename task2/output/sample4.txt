Prediction of Lymph Node Metastasis in
Colorectal Cancer Using Intraoperative
Fluorescence Multi-Modal Imaging
Xiaobo Zhu, He Sun, Yuhan Wang, Gang Hu, Lizhi Shao, Song Zhang, Fucheng Liu, Chongwei Chi, Kunshan He, Jianqiang Tang, Yu An	, Member, IEEE, Jie Tian, Fellow, IEEE, and Zhenyu Liu	, Member, IEEE

Abstract—The diagnosis of lymph node metastasis (LNM) is essential for colorectal cancer (CRC) treatment. The primary method of identifying LNM is to perform frozen sections and pathologic analysis, but this method is labor-intensive and time-consuming. Therefore, combining intraoperative fluorescence imaging with deep learning (DL) methodscanimproveefficiency.Themajorityofrecentstud-
ies only analyze uni-modal fluorescence imaging, which provides less semantic information. In this work, we mainly established a multi-modal fluorescence imaging feature fusion prediction (MFI-FFP) model combining white light, fluorescence, and pseudo-color imaging of lymph nodes for LNM prediction. Firstly, based on the properties of various modalimaging,distinctfeatureextractionnetworksarechosenforfeatureextraction,whichcouldsignificantlyenhance the complementarity of various modal information. Secondly, the multi-modal feature fusion (MFF) module, which combines global and local information, is designed to fuse the extracted features. Furthermore, a novel loss function is formulated to tackle the issue of imbalanced samples, challenges in differentiating samples, and enhancing sample variety. Lastly, the experiments show that the model has a higher area under the receiver operating characteristic (ROC) curve (AUC), accuracy (ACC), and F1 score than the uni-modal and bi-modal models and has a better performance compared to other efficient image classification networks. Our study demonstrates that the MFI-FFP model has the potential to help doctors predict LNM and shows its promise in medical image analysis.
Index Terms—Deep learning, multi-modal imaging, intraoperative fluorescence imaging, colorectal cancer, lymph node metastasis prediction.
I. INTRODUCTION
C
OLORECTAL cancer (CRC) is the third most commonly diagnosed cancer and the third most common cause of cancer-related deaths worldwide [1]. The incidence of CRC has increased in recent years, and it accounts for about 10% of all cancers [2]. Complete resection of the primary tumor and regional lymph nodes is considered the most critical aspect of CRC treatment [3]. The lymph node metastasis (LNM) status significantly influences treatment decisions, including the choice of preoperative neoadjuvant radiotherapy versus surgery [4]. Preoperative conventional imaging methods such as computed tomography (CT) and magnetic resonance imaging (MRI) have some limitations in detecting LNM, such as the risk of radiation, low diagnostic efficiency that requires expert interpretation, etc., and need to be combined with other techniques to confirm the diagnosis [5], [6], [7]. Although, the utilization of lymph node biopsy can more accurately assess LNM; however, it leads to an expensive investment of time, labor, and other costs [8].
In CRC, Zhao et al. explored potential genomic phenotypes associated with deep learning (DL) features that facilitate LNM prediction [9]. Lennard et al. enhanced the prediction performance by utilizing a DL technique to analyze histological whole slide images (WSIs) to predict LNM in CRC [10]. Furthermore, Li and colleagues proposed improving the accuracy of LNM identification through the application of MRI in conjunction with a pre-trained Inception-v3 model on transfer learning [11]. However, the majority of studies on the prediction of LNM in CRC use either preoperative CT, MRI, or pathology imaging, or combine tissue genetic information with deep learning methods for prediction, which significantly raises implementation costs and reduces overall efficiency. Furthermore, these preoperative imaging techniques are incapable of visualizing the complete architecture of the lymph node, and their intraoperative utility is significantly restricted, leading to diminished detection efficiency and accuracy [12]. Nevertheless, intraoperative fluorescence navigation systems enable both surgical navigation during surgery and real-time imaging of multi-modal fluorescence images to visualize tissues [13]. Due to the quick capture time and affordable nature of intraoperative multi-modal fluorescence imaging, the analysis of multi-modal fluorescence imaging not only decreases the time and cost of labor but also has the potential to assist in making clinical decisions during surgery. Hence, we propose integrating DL with intraoperative fluorescent multi-modal imaging to enhance the efficiency and accuracy of intraoperative LNM prediction significantly.
Indocyanine green (ICG) is considered to be more effective in CRC lymph node identification since ICG penetrates relatively deeply into the tissue of the colon compared to other fluorescent tracers [14]. When injected intravascularly, ICG binds to proteins and is transported through the lymphatic system, normally draining to the closest lymph node in 15 minutes or less. It is becoming more typical to employ near-infrared (NIR) fluorescence imaging (FI) to guide surgical guidance following ICG injection [15], [16]. Fluorescenceguided surgery offers advantages such as real-time imaging, high visualization, enhanced contrast, non-invasiveness, precise navigation, and broad applicability, all of which contribute to improved surgical safety, accuracy, and success rates. Typical fluorescence surgical imaging is available in three modes, white light imaging (WLI), fluorescence imaging (FI), and pseudo-color imaging (PCI). Most of the existing studies use DL to analyze FI, mainly for cancer diagnosis or image enhancement, etc., while few studies have been conducted to predict LNM.
Compared to machine learning-based radiomics approaches, DL offers the advantage of automatically learning hierarchical features from raw data, eliminating the need for manual feature extraction, and often achieving superior performance in tasks such as image classification, segmentation, and pattern recognition [17]. In the field of medical image analysis, DL has been widely utilized for analyzing multi-modal images, including CT and MRI. However, most multi-modal medical imaging analysis methods employ the same feature extraction network, which is unable to take into account the specificity of distinct imaging features. In addition, most of the methods for multi-modal medical imaging feature fusion in the intermediate stage of the model directly integrate the features by dimensionally concatenating them or utilizing the Transformer structure [18], but this is not able to focus on the feature information at both the global and local levels.
Most studies have increasingly incorporated DL methods to process intraoperative FI for targeted medical tasks. Shen et al. developed an FL-CNN model to extract relevant information from glioma FI, enhancing the precision of intraoperative glioma diagnosis to assist surgeons in accurately identifying and removing gliomas [19]. Cahill et al. proposed a method combining artificial intelligence (AI) with intraoperative FI video analysis, which was utilized to improve the accuracy of tumor identification for intraoperative tissue classification of colorectal cancer [20]. The growing number of researchers leveraging AI to analyze intraoperative fluorescence imaging or video highlights the advantages of fluorescence-based surgical guidance [21], [22], [23]. Additionally, Xiao et al. developed a method for intraoperative glioma grading by combining various imaging features and introduced DLS-DARTS to fuse multi-modal near-infrared (NIR) fluorescence imaging, resulting in improved predictive outcomes [24]. Multi-modal medical imaging learning can take advantage of data from different medical imaging modalities to achieve comprehensive analysis and diagnosis. Most of the studies analyzed FI, while a few combined FI and WLI but lacked the analysis of PCI. Hence, three intraoperative modality imaging types, WLI, FI, and PCI, are utilized in our study to increase the information on lymph node features in CRC, which improves the prediction performance.
Here, we propose a multi-modal imaging learning framework, called the multi-modal fluorescence imaging feature fusion prediction (MFI-FFP) model, to extract and integrate multi-modal intraoperative fluorescence information, including the three modalities of WLI, FI, and PCI, for LNM prediction in CRC. In this study, a CRC lymph node dataset is collected and prepared specifically for the three modalities of WLI, the first NIR fluorescence window (NIR-I, 700-900 nm) FI, and PCI. First, we design a feature extraction framework comprising three pre-trained branches to extract lymph node features from the tri-modal images: the Inverse Residual Network branch for white light image features, the Residual Network branch for fluorescence image features, and the Transformer Network branch for pseudo-color image features. Subsequently, the integration of global and local feature information from the three modal data is done by a multi-modal feature fusion (MFF) module, which then produces the fusion lymph node features. Moreover, LNM in cancer patients typically exhibit a more pronounced benign-malignant imbalance, with the benign-malignant ratio escalating to 10:1 in extreme instances [25], [26], [27]. Therefore, a novel loss function is tailored to the data characteristics to address the issue of class imbalance and the challenge of distinguishing between classes in the dataset. The main contributions of our work are summarized as follows:
To fully utilize the feature information from each imaging modality, we select three specialized feature extraction branches for the tri-modal images. This approach enhances the model’s ability to capture and characterize features specific to each modality, and experimental results confirm the effectiveness of this design.
To comprehensively utilize the complementary information contained in different modal imaging and provide more comprehensive and accurate imaging characterization, we design a multi-modal feature fusion (MFF) module to enhance the model’s ability to combine multi-modal imaging information for decision-making.
We propose a novel intra-class loss function based on the combination of label-distribution-aware margin (LDAM) loss and Focal loss, which increases the diversity of the data while reducing the impact of the sample imbalance problem.
II. METHODOLOGY
A. Multi-Modal Imaging Feature Extraction Branches
As demonstrated in Fig. 1, three feature extraction branches are built at the beginning of the entire MFI-FFP model to extract WLI, FI, and PCI image features. The advantage of extracting multiple modal images utilizing distinct feature extraction branches is that the feature information of each imaging modality can be fully utilized, potentially enhancing the model’s characterization capacity. To ensure that the primary feature information is retrieved with the least amount of loss, we utilize a background removal method for the WLI data to eliminate superfluous background interference noise. Given that white light imaging (WLI) is characterized by high pixel density and numerous features, we employ an inverted residual structure to efficiently capture spatial details and structural features. This module enhances computational efficiency by reducing the number of operations in the bottleneck layer while preserving critical feature representations, enabling lightweight feature extraction. This makes it well-suited for WLI, which typically demands high-resolution and detailed spatial information. The inverted structure is particularly advantageous for processing high-dimensional images like WLI, as it balances accuracy with computational complexity.
In contrast, a conventional ResNet is used for fluorescence imaging, which has three-channel values resembling the characteristics of grayscale images. ResNet is effective at capturing both fine and coarse features across different depths. Since the spatial variability in fluorescence imaging is generally lower than that in WLI, a standard ResNet is sufficient for extracting the necessary hierarchical features without the need for additional computational optimizations.
PCI data, which includes tissue size, edge feature, and fluorescence intensity information is the result of mapping fluorescence feature information on tissue imaging. To avoid the WLI data losing some edge information during background removal, no background removal procedure is applied to the PCI data in this study. The Vision Transformer (ViT) structure is used for pseudo-color imaging (PCI) due to its ability to capture long-range dependencies within the data. Unlike convolutional networks, the ViT model establishes connections between local and global features, allowing it to focus on multiple parts of the image simultaneously. This capability enables effective capture of both local and global relationships in PCI, which is essential for accurate feature extraction. The extracted features from PCI can effectively complement the WLI and FL features, enhancing the overall feature representation and model performance.
In the feature extraction network branches, the WLI, FI, and
PCI sample spaces are defined as XW = xW1 , xW2 ,..., xWn	,
XF = xF1 , xF2 ,..., xFn	, and XP = x1P, x2P,..., xnP	, respectively, and xWi , xFi , and xiP ∈ RC×H×W. The corresponding inverse residual, residual, and ViT feature extraction branches are defined as functions FIR, FR, and FViT, respectively. Thus, the procedure for extracting features is as follows in (1) to (3).
	fPi = FViT(xiP)	(3)
where fWi , fFi , and fPi are the three modal imaging features corresponding to the lymph node with index i, respectively. In this case, the ViT model is calculated as follows in (4) to (7).
where xclass is the input PCI data, MLP represents the computational process of the multilayer perceptual machine, and MSA represents the computational process of the multi-head attention mechanism.
It is evident how the single-head attention mechanism is calculated for the MSA computation, as demonstrated in (8).
QK T 
Attention(Q, K, V) = soft max	√	V	(8) dk
where the value vector is V, the key vector is K, and the query vector is Q. The single-head attention mechanism extends the computation of the multi-head attention mechanism. The QKV vector matrix is mapped several times by the multi-head attention mechanism, which then combines the vector matrices. The equations (9) and (10) illustrate the computation procedure.
MultiHead(Q, K, V)
	= Cat (head1,··· ,headh)	(9)
		Q	K	· WiV 	(10)
headi = Attention Q · Wi , K · Wi , V
To validate the effectiveness of the designed feature extraction network, Section IV-A demonstrates the structure with experiments.
B. Multi-Modal Imaging Feature Fusion
After extracting the features of the three modal, WLI, FI, and PCI data, feature fusion of the three modal features is required. The spatial alignment of the tri-modal data is ensured during acquisition, and the same size is maintained throughout feature extraction and in the resulting feature maps, ensuring that the features from each modality remain aligned. In the feature fusion module, after summing the dimensions of the tri-modal features, only weighting and multiplication operations are applied at corresponding positions, maintaining feature alignment throughout the entire process. Accordingly, we propose the MFF module to perform global and local-level feature fusion on the three modal features and finally output the fused feature bf , as illustrated in Fig. 2. Initially, an element-wise adding operation along the channel direction is applied to the three retrieved modal characteristics, fW, fF, and fP, as shown in (11).
	fadd = fW ⊕ fF ⊕ fP	(11)
Additionally, fadd needs to be put into the global local feature fusion (GLFF) module. The GLFF module has two branches: the left branch handles global feature interaction, while the right branch handles local feature interaction. The Sigmoid function is subsequently utilized to output the results after the operation has been added together. Local features can capture specific details and unique information, whereas global features are typically able to capture the general global structure and semantic information of the image. To assist the model in comprehending the overall features, the left branch of the GLFF module implements a global average pooling layer to gather global information about the complete feature map. Furthermore, by using point-wise convolution to interchange and integrate data between various spatial locations or channels, both branches simultaneously alter the dimensionality of the feature maps. In this model, global
gobal
features fadd are extracted through Global Average Pooling, summarizing the overall information from the input data and capturing the global context. On the other hand, local features faddlocal are obtained using Point-wise Convolution, which retains spatial details and local patterns in the data. The GLFF module integrates both global and local features, leveraging the summarization capability of global features and the detail-preserving nature of local features to enhance feature representation and improve model performance. The GLFF module process is as follows in (12) to (14).
gobal
where GAP is global average pooling, PWC is point-wise convolution, ξ is the Rectified Linear Unit (ReLU) activation function, σ is the Sigmoid function, and BN is the Batch Normalization (BN) layer. Here, the input feature tensor shape
C′	′	′	C′ × 1 × 1, then restored to
× H × W is compressed to γ
C′ × H′ × W′ using point-wise convolution, which exchanges and integrates information.
The totals of the three modal features are subsequently weighed individually using fadd′ , which is the fusion weight information obtained from the GLFF module’s output. Finally, the adding operation is carried out to complete the fusion of the three modal features, as in
	bf = ( fadd′	⊗ fW) ⊕ ((1 − fadd′	) ⊗ fF) ⊕ ( fadd′	⊗ fP).
(15)
Additionally, to better adapt to various tasks and contexts, the relative prominence of certain modal elements can be dynamically changed. The GLFF output can assist the model in narrowing its focus to the most pertinent data, and by weighting and summing various modal features, the redundancy between features can be minimized. Moreover, the FI has fewer similar features to WLI and PCI, so a oneminus weighting operation is adopted. The MFF enhances the model’s capacity for generalization, increases its efficiency, and prevents needless computational and storage overheads.
C. Loss Function
In colorectal cancer patients, metastatic lymph nodes constitute only a small fraction of the total lymph nodes, resulting in a significant imbalance between benign and malignant samples. To address this imbalance, LDAM Loss [28] is employed in this study. Additionally, due to the similarities between benign and malignant lymph nodes observed in intraoperative fluorescence imaging, distinguishing between them can be challenging. To tackle this issue, we use Focal Loss [29]. Finally, we design a novel intra-loss function to enhance sample diversity, further improving model performance by building on the combination of the previous losses. Thus, the new loss function consists of LDAM Loss, Focal Loss, and Intra-Loss, designed to enhance model prediction performance by combining the strengths of these three loss functions.
1) LDAM Loss: The LDAM loss aims to minimize generalization boundaries based on margin. To enhance the generalization performance of the minority class, the classification boundary can be shifted toward the majority class. This loss can be employed with a priori strategies for training with class-imbalanced data, including reweighting or resampling, in place of the traditional cross-entropy function during training [28]. The specific formula is shown in (16).
	N		eziy−1y	
X
LLDAM = −	log y	j 	(16) i=1	ezi −1y + Pj̸=y ezi
where Cˆ is the hyperparameter that needs to be adjusted, n j y is the class j sample size, k is the number of classes and zi is the yth output of the model for the ith sample, which belongs to class y. Moreover, LDAM is reweighted by frequency in order to enhance its performance on class-imbalance learning. In our loss function, the reweighted LDAM is used, as in
e
	reweighted	XN		ziy−1y	
LLDAM	= −	ωi log y	j .	(18) i=1	ezi −1y + Pj̸=y ezi
Focal Loss: Focal loss effectively suppresses the influence of easily classified samples on the loss by reducing their weight and increasing the weight of hard-to-classify samples, thus enhancing the model’s focus on difficult-to-classify samples [29]. It addresses class imbalance issues and improves model performance on minority classes. The formula is shown blew
N
	LFL = −Xωi (1 − pi)κ log(pi)	(19)
i=1 where (1 − pi)κ is a modulating factor, and κ is a hyperparameter needed to be setting. The higher value of (1 − pi)κ means that the model will pay more attention to hard samples with small values of pi.
Intra Loss: In order to bring the intra-class samples closer together and increase the diversity of the samples, we propose the Intra-loss. Initially, the Pearson correlation coefficient (PCC) is employed to determine the similarity between neighboring samples within a class, as shown in
Cov( f (xi), f (xi+1))
ρxixi+1 =		(20) δ ( f (xi))δ ( f (xi+1))
where f (xi) and f (xi) represent the output of the model for samples xi and xi+1, respectively; Cov(·) is the covariance function; δ(·) is the standard deviation function. Therefore, the average similarity of each class defines the feature diversity φm of class m, as shown blew
P	m ρxi xi+1 xi∈φ
ρm =		(21) nm
where nm is the number of samples belonging to class m, and φm is the set of sample space on class m. As illustrated in (22), distinct classes are required to have similar data distributions in intra-class losses to concurrently tackle the class imbalance problem of data scarcity and data density.
	LIntra =	X		(22)
m,k∈φ and m̸=k
The final form of the loss function is shown below
	LAll = LreweightedLDAM	+ αLFL + βLIntra.	(23)
The proposed novel loss function has combined the aforementioned three loss functions, which will address the issues of sample imbalance and differentiation difficulty while also increasing the intra-class variety.
III. EXPERIMENTAL SETTINGS
A. Data
The intraoperative fluorescence dataset employed in this study contains three modalities of WLI, FI, and PCI data, where FI data are NIR-I fluorescence. The lymph node samples used in this study are isolated during colorectal cancer surgeries and imaged intraoperatively. For each lymph node, we capture three types of modality images: white-light imaging (WLI), fluorescence imaging (FI), and pseudo-color imaging (PCI) using the intraoperative fluorescence device (DPM-ENDOSCOPE-3D06 and DPM-0PENCAM-02, Zhuhai Dipu Medical Technology Co., Ltd.). The training and test datasets are provided by Beijing Fengtai Hospital, which supplied multi-modal fluorescence images of isolated lymph nodes from 49 patients. A total of 996 lymph nodes are analyzed, 87 of which are metastatic, while the remaining 909 are non-metastatic, resulting in a negative-to-positive sample ratio of approximately 10:1. In total, 2,988 multimodal images are collected, with 996 images per modality. The dataset is split into train and test sets in the trials using a 7:3 ratio, and the three modal images adhere to the same dataset division guidelines.
To verify the model’s generalization performance and robustness, we acquire multi-modal lymph node images from 19 colorectal cancer patients at the Cancer Hospital, Chinese Academy of Medical Sciences. This external validation set consists of 270 lymph nodes, yielding a total of 810 images, of which 76 nodes are metastatic. Intraoperative isolated lymph nodes are placed on medical placement cloths, in order to reduce the interference of the background on the WLI data, we sequentially perform the background removal operation on all WLI image data via the Photoshop software. To avoid inadvertent removal of the edge information of lymph nodes in the WLI, the background removal operation was not carried out on the PCI data so that the feature extraction network could perform the supplementation corresponding to the missing information during the model training. In the beginning, the image size is adjusted to 224 × 224, and random horizontal flip and random rotation operations are carried out.
B. Implementation Details
The three modal imaging data sizes input to the model are (B,3,224,224), where B is the batch size. In the feature extraction network, the branching structure for extracting the WLI data is a convolutional layer with an output dimension of 32, immediately followed by 17 inverted residual modules, and finally a convolutional and fully connected layer with an output feature map shape of (B,512,7,7). All the above convolutional layers are immediately followed by the BN layer and the ReLU6 activation function. The branching structure of the extracted FI features uses the network structure of ResNet18 [30] throughout, and the shape of the output feature map is (B,512,7,7). The feature branching structure of the extracted PCI is the ViT structure, which is firstly sliced and mapped to the data, then converted to the form of a long sequence of vectors, immediately followed by stacking of 12 Transformer modules, and finally outputting a feature map with the dimensions (B,512,7,7). Pre-trained weights trained on the ImageNet dataset have been introduced into the complete feature extraction network to speed up model convergence and increase model accuracy.
The three modal feature dimensions C′ × H′ × W′ input to the MFF module are (B,512,7,7), followed by elementwise addition, and the output feature dimensions after passing through the MFF module is (B,512,7,7), where the hyperparameter γ in the GLFF module is 8. The final multi-layer perception (MLP) structure of the model is an adaptive pooling layer with an output dimension of (B,512,1,1), followed by two fully connected (FC) layers mapping the feature dimensions from 512 to 2. Moreover, the two FC layers have a ReLU activation function applied and a dropout layer to prevent over-fitting.
The hyperparameters during model training are set as follows: learning rate (lr) is set to 0.001; weight decay (wd) is set to 0.003; batch size (B) is 32; and the dropout layer hyperparameter is set to 0.3. The Adam optimizer is chosen as the optimizer for model training. A total of 100 epochs are set up for the training process. The hyperparameter settings in the loss function align with those in references [28] and [29]. The hyperparameters in the loss function LAll are set as follows: the weight wi of LreweightedLDAM and LFL are both set to 0.85; Cˆ in LreweightedLDAM is set to 0.5; and κ in LFL is set to 2.0. In Section IV-E, the sensitivity analysis of the hyperparameters indicates that α and β should be set to 1 for optimal model performance. The training process for all experiments is implemented using Pytorch-2.1.0 and is performed on the NVIDIA RTX 4090 GPU with 24 GB of memory.
C. Evaluation Metrics
To validate the performance of our proposed MFI-FFP model for the prediction of colorectal cancer, the main assessment metrics employed are the area under the receiver operating characteristic (ROC) curve (AUC), accuracy (ACC), and F1 score. The ROC curve is a curve with the true positive rate (TPR) as the y-axis and the false positive rate (FPR) as the x-axis. The AUC is the area under the ROC curve, which indicates the model’s classification performance under different thresholds. The larger the AUC means the better the model’s performance.
IV. RESULTS
A. Comparison of Different Feature Extraction Branches
To evaluate the performance of different network architectures as feature extraction branches for the three modal imaging, we extracted WLI, FI, and PCI data features using the residual block (RB) structure, the inverted residual block (IRB) structure, and the ViT structure, respectively. As shown in Table I, the results of applying the same branching structure to the three modal data are also compared. Consequently, all comparison experiments are evaluated on the test dataset and external validation set across 5 runs. The median of these 5 runs of results is selected for presentation and 95%
confidence intervals (CI) are calculated using 1000 trials of the Bootstrap approach.
The comparison results of the three modal imaging data extracted by different feature extraction branches show that extracting WLI data features using IRB structure, FI data features using RB structure, and PCI data features using ViT structure demonstrated the best performance, with the AUC of 0.8294 (95% CI 0.7813-0.8431), the ACC of 0.8669 (95% CI 0.8487-0.8967), and the F1 score of 0.4999 (95% CI 0.4612-0.5174) on the test set. Similarly, the model demonstrates strong performance on the external validation set, further confirming its generalization capability and robustness.
Fig. 3. The ROC curves for comparison results of uni-modal and multimodal images. (a) The ROC curves for the results of the comparison between the tri-modal and uni-modal data on the test set. (b) The ROC curves for the results of the comparison between the tri-modal and unimodal data on the validation set. (c) The ROC curves for the results of the comparison between the tri-modal and bi-modal data on the test set. (d) The ROC curves for the results of the comparison between the tri-modal and bi-modal data on the validation set.
Furthermore, there is relatively high efficiency in the experimental results when exchanging the WLI and PCI data feature extraction branches, which achieves the AUC of 0.8054 (95% CI 0.7935-0.8283), the ACC of 0.8672 (95% CI 0.84620.8882), and the F1 score of 0.4236 (95% CI 0.3920-0.4542) on the test set. The ACC on the external validation set reaches 0.8296. The structure of the PCI data is similar to that of the WLI data, but it is mixed with some background noise and lacks some lymph node texture information after fluorescence information mapping. As a result, the global features cannot be effectively extracted using the IRB structure, which slightly reduces the overall performance of the model. Additionally, the last three rows of Table I demonstrate that even though similar features can be extracted using the same feature extraction structure in all three modalities, the unique features of each modality cannot be adequately represented.
B. Comparison of Single-Modal, Multi-Modal Imaging
To assess the effectiveness of fusing feature information from tri-modal imaging compared to fusing feature information from bi-modal or even uni-modal imaging, some comparative experiments using only bi-modal or uni-modal imaging are conducted in this study. As shown in Table II, the MFI-FFP model can more comprehensively learn the different feature information of the current corresponding lymph node by combining the feature information from the three modal imaging, WLI, FI, and PCI, resulting in a more effective way to distinguish LNM. The experimental results demonstrate that the network learns best and the model performs optimally when the three modal imaging data serve as inputs, obtaining the ACC of 0.8782 (95% CI 0.8487-0.8967), the AUC of
0.8294 (95% CI 0.7813-0.8431), and the F1 score of 0.4999
Fig. 4. The ROC curves of the results of our model compared to other classification models. (a) The ROC curves of the results of our model compared to other classification models with features concatenated in dimension on the test set. (b) The ROC curves of the results of our model compared to other classification models with features concatenated in dimension on the validation set. (c) The ROC curves of the results of our model compared to other classification models with attention-based features fusion method on the test set. (d) The ROC curves of the results of our model compared to other classification models with attention-based features fusion method on the validation set.
(95% CI 0.4612-0.5174) on the test set. In the bi-modal imaging data feature fusion experiments, the model effect of combining WLI and FI demonstrates better performance, achieving an AUC of 0.7860 (95% CI 0.7678-0.8174) and the ACC of 0.8192 (95% CI 0.8087-0.8561) on the test set, and an AUC of 0.7862 (95% CI 0.7501-0.8218) and the ACC of 0.7889 (95% CI 0.7407-0.8133) on the validation set, which indicates that the WLI and FI data contain minimal feature overlap and possess distinct information. Furthermore, the results from the unimodal imaging comparison reveal that the model trained on WLI data alone performed significantly better than those trained on the other two modalities. This not only demonstrates that the WLI data contains a substantial amount of valid lymph node feature information but also indirectly highlights the importance of background removal.
Fig.3 illustrates the ROC curves comparing the tri-modal imaging with the bi-modal and uni-modal imaging experiments on both the test set and validation set. It can be concluded that the classification performance of the tri-modal model is the most superior, and the uni-modal model performs the weakest. The results demonstrate that the characteristics of lymph nodes can be captured as comprehensively as possible using the information from the tri-modal imaging data, which is much more effective than the bi-modal and uni-modal imaging data and directly enhances the model performance and accuracy.
C. Comparison With Various Classification Models
Comparison of other superior classification models with our proposed MFI-FFP model on the task of intraoperative fluorescence LNM prediction in CRC can visually and efficiently illustrate the excellent performance of our model. Since there are no multi-modal models that specifically deal with intraoperative fluorescence data from lymph nodes in CRC, we choose some image classification models that exhibit superior performance on natural images as comparison models. Several outstanding research works have been conducted on natural image classification tasks; among these, the Resnet18 [30], the Swin-Transformer [31], the ConvNeXt [32], the Efficientnet-v2 [33], and the Mobilenet-v3 [34] are selected as comparison models.
All comparative models are pre-trained on the ImageNet dataset to boost the convergence of these models. The three modal features produced by the comparative models are fused for each of the two fusion methods, one for concatenating in the dimensional direction and one for attention-based fusion (WLI features serve as the query tensors, FI features as the key tensors, and PCI features as the value tensors). The experiment compares these two common fusion methods to prove our model’s performance. The loss functions for all comparison models training are all set to a weighted cross-entropy loss function with weights of 0.85. The five comparison models include both the traditional convolutional network structure and the Transformer structure, which has been quite remarkable in recent years, and the final comparison results are shown in Tables III and IV. The results indicate that the effect of our model works best on the intraoperative fluorescence tri-modal imaging dataset, followed by the training results of the Mobilenet-v3 model with attention-based fusion, which also demonstrated good results, achieving an AUC of 0.7712 (95% CI 0.7369-0.7915) and an ACC of 0.8266 (95% CI 0.8038-0.8559) on the test set, and an AUC of 0.7698 (95% CI 0.7257-0.7988) and an ACC of 0.7630 (95% CI 0.73150.7953) on the validation set. The floating point operations per second (FLOPs) of our model consume less computational resources compared to the pure Transformer structure model, but this is slightly worse than the convolutional network structure model, which is caused by the fact that the Transformer structure with a large number of parameters is also included in our model. Fig. 4 shows the ROC curves comparing the results of multiple models using different fusion methods, visually illustrating the superior performance of our model. All the above comparison experiments are trained on the dataset for 5 runs to avoid serendipity in the experimental results.
D. Ablation Experiments
In addition to conducting comparative experiments to validate the efficient performance of our model, we also design three ablation studies to evaluate the performance impact of the innovative part on the MFI-FFP model. The first ablation study is performed on each part of the losses in the designed loss function to verify the impact of each of these loss parts on the model training process, which is based on the presence of the fusion module MFF. In addition, the second ablation study is performed on the designed loss function and the fusion module MFF to verify the impact of each component on our model. The final ablation experiment demonstrates the effectiveness of different components by removing the feature extraction branch and the MFF module, confirming the significance of each in enhancing the model’s overall performance.
Ablation Study for Analyzing the Loss Function: Table V presents the general results of the ablation experiment conducted on each of the three loss parts, LreweightedLDAM , LFL, and LIntra in the first ablation study. The results indicated that all three loss components of the loss function LAll applied during model training performed the best, achieving the highest AUC and ACC. The ablation experiment with LIntra alone as the entire loss function performs the worst, this is because the loss is not responsible for solving the sample imbalance problem and the problem of difficulty in distinguishing between samples. Applying only the LreweightedLDAM and LFL losses can reach an AUC of 0.7576 (95% CI 0.7211-0.7841) on the test set and an AUC of 0.8005 (95% CI 0.7707-0.8373) on the validation set, and the result after adding the LIntra loss shows that the LIntra loss is effective in improving the diversity of the samples, which indirectly enhances the overall model effect. The sample imbalance issue in the validation set is not as pronounced as in the test set, and the model’s performance on the validation set is superior to that on the test set. This indirectly highlights the effectiveness of the designed loss function in addressing sample imbalance. Additionally, the impact increase is more noticeable when a single LreweightedLDAM or LFL is combined with LIntra loss, respectively.
Ablation Study for the Loss Function and the MFF Module: Table VI demonstrates the results of the second ablation study, where the model that uses neither the newly designed loss function (applying a cross-entropy loss function with a weight of 0.85) nor the MFF has the worst performance, with the AUC of only 0.6933 (95% CI 0.6503-0.7241) on the test set and the AUC of 0.6518 (95% CI 0.6163-0.6789) on the validation set. Moreover, it is evident from the experimental results that the model has a significant performance improvement effect by applying the new loss function alone and the MFF fusion module alone. The simultaneous usage of the newly designed loss function and the fusion module MFF enables the model to reach its highest performance.
Ablation Study for the Feature Extraction Branches and the MFF Module: Table VII presents the results of the final ablation experiment, showing that the worst performance is observed when the MFF module is removed, while the three feature extraction branches maintain the same network structure. Once the MFF module was added, all models experienced a performance improvement, demonstrating its effectiveness. Furthermore, when our custom-designed feature extraction branches are used as the backbone network, the overall results surpass those of other branch designs. This validates the design of the inverse residual network for extracting WLI features, the residual network for FI features, and the ViT network for PCI features.
E. Hyperparameter Sensitivity Analysis
To verify the impact of the loss function hyperparameters on model performance, we set the values of α and β within the range [0.01, 0.05, 0.1, 0.5, 1.0] for hyperparameter sensitivity analysis. As shown in Fig. 6, small values for α and β negatively affect overall model performance, resulting in an AUC of only about 0.75 on the test set, with similar poor performance on the validation set. In contrast, the model performs better when α and β are close to 1, indicating improved results when the weights of the three individual losses in the total loss function are similar. This demonstrates that the model is sensitive to hyperparameters α and β, achieving optimal performance when both are set to 1.
F. Comparison With Clinical Experts
To assess the clinical value of the model, we invited five clinicians with experience in colorectal cancer treatment to evaluate lymph node metastasis based on multi-modal fluorescence images from the external validation set. This group included experts 1 and 4, who have less than 10 years of

Fig. 6. Hyperparameter sensitivity analysis. (a) Results of the sensitivity analysis of hyperparameters α and β in the loss function on the test set. (b) Results of the sensitivity analysis of hyperparameters α and β in the loss function on the validation set.
TABLE VIII
COMPARISON RESULTS OF CLINICAL EXPERTS WITH OUR MODEL ON
THE EXTERNAL VALIDATION SET

clinical experience, and experts 2, 3, and 5, who have more than 10 years of clinical experience. The clinicians provided a binary judgment on whether a lymph node is metastatic based on its tri-modal imaging. The evaluation metrics include sensitivity(SEN), specificity(SPE), accuracy(ACC), precision(PREC), and F1 score. As shown in Table VIII, the results demonstrate that our model outperforms the clinicians in sensitivity, accuracy, precision, and F1 score. Notably, expert 1 achieved a specificity of 0.9381, but a sensitivity of only 0.1447, which is attributed to his tendency to classify most lymph nodes as non-metastatic. Overall, the results demonstrate that our model is more effective at detecting potential features in images compared to clinical experts, making it a more accurate and efficient approach than traditional imaging-based clinical judgment.
G. Interpretative Visualization
To increase the interpretability of our model network, we utilize the Gradient-weighted class activation mapping (GCAM) method [35], known as a technology for the visualization and interpretation of deep learning models, to increase the understanding of the feature extraction and feature fusion layers of the model. Fig. 5 displays two samples: one with LNM and the other without. The first three columns display the three modal images of the lymph node samples. Columns 4 to 6 display the results of GCAM heat map mapping when the three modal imaging features are extracted on each of the three feature extraction layers, and the final column reveals the GCAM heat map mapped image on the WLI image on the fusion layer.
The results reveal that the feature extraction branch of the convolutional network structure pays more attention to information such as the surface texture of the image, while the feature extraction branch of the Transformer structure pays more attention to the global location information supplementing the lymph node edge feature information. Furthermore, the fused features are combined with modifications to the region of interest by the feature fusion layer, which highlights and concretizes the characteristics in crucial places. For images of lymph nodes that have metastasized, our model pays more attention to the weird irregular texture and shape features in the samples.
V. DISCUSSION AND CONCLUSION
In this study, we propose the MFI-FFP model for LNM prediction in CRC using intraoperative fluorescence multimodal imaging. Our model first extracts three modal data features, followed by the fusion of the extracted three modal features, and eventually performs LNM in CRC prediction based on the fused features. The suggested MFI-FFP model exhibited promise for real-time LNM diagnosis in the future by performing satisfactorily in the task of predicting LNM in CRC using intraoperative fluorescence multi-model imaging.
In the feature extraction part of our model, the corresponding special extraction branches are set according to the characteristics of the three modal data, respectively. Furthermore, experimental results indicate it is frequently more effective in building distinct feature extraction structures based on various modal imaging feature information than in using the same structure. The three modal features extracted in the previous stage are features fused in the MFF module in the model, which incorporates global and local features to boost model performance. Furthermore, there is a significant sample imbalance issue because the majority of the lymph nodes in the data sample are benign, and relatively few of them are malignant. According to the above problems, we design a novel loss function, in which the LDAM loss alleviates the sample imbalance problem, the focal loss reduces the difficulty in distinguishing the samples, and the proposed intra-loss increases the diversity of the samples and thus improves the performance of the model.
To predict LNM in CRC, our work leverages intraoperative fluorescence multi-modal imaging. By combining three modal imaging characteristics, the model can effectively learn lymph node feature information and exhibit outstanding prediction ability. Since little research has combined deep learning with intraoperative FI for identifying LNM, utilizing deep learning to analyze intraoperative FI for various downstream tasks has far-reaching implications and potential in the medical field.
REFERENCES
R. L. Siegel, N. S. Wagle, A. Cercek, R. A. Smith, and A. Jemal,
“Colorectal cancer statistics, 2023,” CA, A Cancer J. Clinicians, vol. 73, no. 3, pp. 233–254, May 2023.
E. Dekker, P. J. Tanis, J. L. A. Vleugels, P. M. Kasi, and M. B. Wallace, “Colorectal cancer,” Lancet, vol. 394, no. 10207, pp. 1467–1480, Oct. 2019.
X. Guan et al., “Optimal examined lymph node number for accurate staging and long-term survival in rectal cancer: A population-based study,” Int. J. Surg., vol. 109, no. 8, pp. 2241–2248, Aug. 2023.
G. J. Chang, M. A. Rodriguez-Bigas, C. Eng, and J. M. Skibber, “Lymph node status after neoadjuvant radiotherapy for rectal cancer is a biologic predictor of outcome,” Cancer, vol. 115, no. 23, pp. 5432–5440, Dec. 2009.
Y. Erog˘lu, M. Yildirim, and A. Çinar, “Convolutional neural networks based classification of breast ultrasonography images by hybrid method with respect to benign, malignant, and normal using mRMR,” Comput. Biol. Med., vol. 133, Jun. 2021, Art. no. 104407.
J. Li et al., “Dual-energy CT–based deep learning radiomics can improve lymph node metastasis risk prediction for gastric cancer,” Eur. Radiol., vol. 30, no. 4, pp. 2324–2333, Apr. 2020.
D. Dong et al., “Deep learning radiomic nomogram can predict the number of lymph node metastasis in locally advanced gastric cancer: An international multicenter study,” Ann. Oncol., vol. 31, no. 7, pp. 912–920, Jul. 2020.
M. Ahmed, A. D. Purushotham, and M. Douek, “Novel techniques for sentinel lymph node biopsy in breast cancer: A systematic review,” Lancet Oncol., vol. 15, no. 8, pp. e351–e362, Jul. 2014.
J. Zhao et al., “Deep learning radiomics model related with genomics phenotypes for lymph node metastasis prediction in colorectal cancer,” Radiotherapy Oncol., vol. 167, pp. 195–202, Feb. 2022.
L. Kiehl et al., “Deep learning can predict lymph node status directly from histology in colorectal cancer,” Eur. J. Cancer, vol. 157, pp. 464–473, Nov. 2021.
J. Li et al., “Deep transfer learning based on magnetic resonance imaging can improve the diagnosis of lymph node metastasis in patients with rectal cancer,” Quant. Imag. Med. Surgery, vol. 11, no. 6, pp. 2477–2485, Jun. 2021.
C. Chi et al., “Intraoperative imaging-guided cancer surgery: From current fluorescence molecular imaging methods to future multi-modality imaging technology,” Theranostics, vol. 4, no. 11, pp. 1072–1084, 2014.
K. Wang et al., “Fluorescence image-guided tumour surgery,” Nature Rev. Bioeng., vol. 1, no. 3, pp. 161–179, Feb. 2023.
T. A. Burghgraef, A. L. Zweep, D. J. Sikkenk, M. H. G. M. van der Pas, P. M. Verheijen, and E. C. J. Consten, “in vivo sentinel lymph node identification using fluorescent tracer imaging in colon cancer: A systematic review and meta-analysis,” Crit. Rev. Oncol./Hematol., vol. 158, Feb. 2021, Art. no. 103149.
S. H. Emile et al., “Sensitivity and specificity of indocyanine green nearinfrared fluorescence imaging in detection of metastatic lymph nodes in colorectal cancer: Systematic review and meta-analysis,” J. Surgical Oncol., vol. 116, no. 6, pp. 730–740, Nov. 2017.
S. R. Thammineedi et al., “Fluorescence-guided cancer surgery—A new paradigm,” J. Surgical Oncol., vol. 123, no. 8, pp. 1679–1698, Mar. 2021.
M. Avanzo et al., “Machine and deep learning methods for radiomics,” Med. Phys., vol. 47, no. 5, pp. e185–e202, May 2020.
O. Dalmaz, M. Yurt, and T. Çukur, “ResViT: Residual vision transformers for multimodal medical image synthesis,” IEEE Trans. Med. Imag., vol. 41, no. 10, pp. 2598–2614, Oct. 2022.
B. Shen et al., “Real-time intraoperative glioma diagnosis using fluorescence imaging and deep convolutional neural networks,” Eur. J. Nucl. Med. Mol. Imag., vol. 48, no. 11, pp. 3482–3492, Oct. 2021.
R. A. Cahill et al., “Artificial intelligence indocyanine green (ICG) perfusion for colorectal cancer intra-operative tissue classification,” Brit. J. Surg., vol. 108, no. 1, pp. 5–9, Jan. 2021.
S. Zhuk et al., “Perfusion quantification from endoscopic videos: Learning to read tumor signatures,” in Proc. 23rd Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI), Lima, Peru. Cham, Switzerland: Springer, Oct. 2020, pp. 711–721.
S.-H. Park, H.-M. Park, K.-R. Baek, H.-M. Ahn, I. Y. Lee, and
G. M. Son, “Artificial intelligence based real-time microcirculation analysis system for laparoscopic colorectal surgery,” World J. Gastroenterol., vol. 26, no. 44, pp. 6945–6962, Nov. 2020.
Z. Ma, F. Wang, W. Wang, Y. Zhong, and H. Dai, “Deep learning for in vivo near-infrared imaging,” Proc. Nat. Acad. Sci. USA, vol. 118, no. 1, Jan. 2021, Art. no. 2021446118.
A. Xiao et al., “Intraoperative glioma grading using neural architecture search and multi-modal imaging,” IEEE Trans. Med. Imag., vol. 41, no. 10, pp. 2570–2581, Oct. 2022.
W. H. Steup, Y. Moriya, and C. J. H. van de Velde, “Patterns of lymphatic spread in rectal cancer. A topographical analysis on lymph node metastases,” Eur. J. Cancer, vol. 38, no. 7, pp. 911–918, May 2002.
J. Kang et al., “LASSO-based machine learning algorithm for prediction of lymph node metastasis in T1 colorectal cancer,” Cancer Res. Treatment, vol. 53, no. 3, pp. 773–783, Jul. 2021.
S.-E. Kudo et al., “Artificial intelligence system to determine risk of T1 colorectal cancer metastasis to lymph node,” Gastroenterology, vol. 160, no. 4, pp. 1075–1084, Sep. 2020.
K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning imbalanced datasets with label-distribution-aware margin loss,” in Proc. Adv. Neural Inf. Process. Syst., vol. 32, 2019, pp. 1–13.
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense object detection,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2980–2988.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770–778.
Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 10012–10022.
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A ConvNet for the 2020s,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2022, pp. 11976–11986.
M. Tan and Q. V. Le, “EfficientNetv2: Smaller models and faster training,” in Proc. Int. Conf. Mach. Learn., 2021, pp. 10096–10106.
S. Qian, C. Ning, and Y. Hu, “MobileNetV3 for image classification,” in Proc. IEEE 2nd Int. Conf. Big Data, Artif. Intell. Internet Things Eng. (ICBAIE), Mar. 2021, pp. 490–497.
R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-CAM: Visual explanations from deep networks via gradient-based localization,” in Proc. IEEE Int. Conf. Comput. Vis.
(ICCV), Oct. 2017, pp. 618–626.